ansible-playbook [core 2.15.2]
  config file = /workspaces/cicd-pipeline-project/ansible/ansible.cfg
  configured module search path = ['/home/cicd-pipeline/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']
  ansible python module location = /home/cicd-pipeline/.local/lib/python3.10/site-packages/ansible
  ansible collection location = /home/cicd-pipeline/.ansible/collections:/usr/share/ansible/collections
  executable location = /home/cicd-pipeline/.local/bin/ansible-playbook
  python version = 3.10.8 (main, Nov 16 2022, 17:55:27) [GCC 9.4.0] (/home/cicd-pipeline/.python/current/bin/python3)
  jinja version = 3.1.2
  libyaml = True
Using /workspaces/cicd-pipeline-project/ansible/ansible.cfg as config file
setting up inventory plugins
Loading collection ansible.builtin from 
host_list declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Skipping due to inventory source not existing or not being readable by the current user
script declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
auto declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Skipping due to inventory source not existing or not being readable by the current user
yaml declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Skipping due to inventory source not existing or not being readable by the current user
ini declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Skipping due to inventory source not existing or not being readable by the current user
toml declined parsing /etc/ansible/hosts as it did not pass its verify_file() method
Loading collection community.general from /home/cicd-pipeline/.ansible/collections/ansible_collections/community/general
Loading collection amazon.aws from /home/cicd-pipeline/.ansible/collections/ansible_collections/amazon/aws
Loading collection kubernetes.core from /home/cicd-pipeline/.ansible/collections/ansible_collections/kubernetes/core
Loading callback plugin default of type stdout, v2.0 from /home/cicd-pipeline/.local/lib/python3.10/site-packages/ansible/plugins/callback/default.py
Skipping callback 'default', as we already have a stdout callback.
Skipping callback 'minimal', as we already have a stdout callback.
Skipping callback 'oneline', as we already have a stdout callback.

PLAYBOOK: helm-charts.yml ******************************************************
Positional arguments: playbooks/helm-charts.yml
verbosity: 4
private_key_file: /workspaces/cicd-pipeline-project/ansible/cicd-pipeline-ec2.key
remote_user: ubuntu
connection: smart
timeout: 10
become_method: sudo
tags: ('all',)
inventory: ('/etc/ansible/hosts',)
extra_vars: ('helm_action=install',)
forks: 5
1 plays in playbooks/helm-charts.yml

PLAY [Provision EKS Cluster and Deploy Helm Charts] ****************************

TASK [Validate helm_action variable] *******************************************
task path: /workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml:28
ok: [localhost] => {
    "changed": false,
    "msg": "All assertions passed"
}

TASK [Apply Terraform Configuration] *******************************************
task path: /workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml:36
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: cicd-pipeline
<127.0.0.1> EXEC /bin/sh -c 'echo ~cicd-pipeline && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/cicd-pipeline/.ansible/tmp `"&& mkdir "` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988 `" && echo ansible-tmp-1691360200.7076132-181395-234672497488988="` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988 `" ) && sleep 0'
Using module file /home/cicd-pipeline/.ansible/collections/ansible_collections/community/general/plugins/modules/terraform.py
<127.0.0.1> PUT /home/cicd-pipeline/.ansible/tmp/ansible-local-181374erbyjfj5/tmpl8uclx0t TO /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988/AnsiballZ_terraform.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988/ /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988/AnsiballZ_terraform.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '/home/cicd-pipeline/.python/current/bin/python3 /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988/AnsiballZ_terraform.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360200.7076132-181395-234672497488988/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "command": "/home/cicd-pipeline/.asdf/shims/terraform apply -no-color -input=false -auto-approve -lock=true /tmp/tmphheklysp.tfplan",
    "invocation": {
        "module_args": {
            "backend_config": null,
            "backend_config_files": null,
            "binary_path": null,
            "check_destroy": false,
            "complex_vars": false,
            "force_init": true,
            "init_reconfigure": false,
            "lock": true,
            "lock_timeout": null,
            "overwrite_init": true,
            "parallelism": null,
            "plan_file": null,
            "plugin_paths": null,
            "project_path": "/workspaces/cicd-pipeline-project/terraform/eks-cluster",
            "provider_upgrade": false,
            "purge_workspace": false,
            "state": "present",
            "state_file": null,
            "targets": [],
            "variables": null,
            "variables_file": [
                "/workspaces/cicd-pipeline-project/terraform/eks-cluster/.tfvars"
            ],
            "variables_files": [
                "/workspaces/cicd-pipeline-project/terraform/eks-cluster/.tfvars"
            ],
            "workspace": "default"
        }
    },
    "outputs": {
        "cluster_certificate_authority_data": {
            "sensitive": false,
            "type": "string",
            "value": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJUHhyOE54Q2hFb0F3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpBNE1EWXlNVFF3TVRGYUZ3MHpNekE0TURNeU1UUXdNVEZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURTSmxla3d6MGF3RUUzNTZMV1V4S0hkQ3pWVm9oZWRwT29iV3Q4N3JVM2ZpWk5LVG42V2lGZ2ZTZG4KOGNlamJmOFladDhRVkxZTVhsSFNwVTdtcVZzS3J5Q0xtR0p5UGJ6aDVJbWcvVCs4RU9IWmtKbjVPUFdZcS96TApGY1ZneGNkN2V1b2R6L2ZnYzQwMkpGdkpnZDhSWTFLUi84R1B3czZ5MkV4YnN4b2F1M0d1SkpTMFdrdGRuem56CjRmL2ZXU0cxOFc5bUJ5MGQ1VE1USGxvMWhkb2oybEZzTmdGWXA2eElpMzRDZlhHaGlHeFVEVjJ2NVZoZXR6TUwKdXdvdjBhSkJHNU9BbmtNUllyYVZVMTVWOEFyblpDUmhzcDVWMUtsdEFYdG5aSXIxZ01hbkZFK0dRNXAzNHhxQgozYkdhVGtBcGp3UHpUVFphYUtYOVNEclZsOXRMQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJSTndpaE1yMEFjL1Ywd20yZW9ScjF6SzBSRXdUQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQ3NuaG9aeWJJYgppa0lGUFN0K1lBRkxDYjdMdEpYNWJxd0FuSDNnT2tkQkNNMG9QR2RLRUZBNGU0MDZhWGxuSWxTLytnbG5KbVF5Ck4za0padGIzbXJkZ3ZkRE5wcE8vQUJRTXVQakljNFhpN1ppTXhuckJoV29hZkMyaXhBb3huNkxDdHhMbUc0Q3IKbzR1TXpCWUdBRkMvZjNRYUFkNHppMWcvWHhGdlNVS20vRnNIbXRmNlhpMmU1UkhHNGh3UVBZOU9xVTVSZDhGYgoxUEFTK2RlVHFrdngrSVIwbWhJUzJXL2lReW8xNWlycitQVGtiQXRsckZQUmdYY2JuTjB3d1U4c2xvTkgzSDlFCndqTG8yRjMvT3g0S0RRUEFMd1NhQ2ZRdnJ1Z05VcmVWVTJPVHQvczk0WE5MN1Z1bUNDVlExRTg2UitKdXJiMHMKZmZxQ21YZVlzT3p6Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K"
        },
        "cluster_endpoint": {
            "sensitive": false,
            "type": "string",
            "value": "https://46053C82096527C9C95ED11748A3B3A0.gr7.ap-south-1.eks.amazonaws.com"
        },
        "cluster_iam_role_name": {
            "sensitive": false,
            "type": "string",
            "value": "cicd_pipeline_cluster20230806213529231200000001"
        },
        "cluster_security_group_id": {
            "sensitive": false,
            "type": "string",
            "value": "sg-0bb10e125879cf48f"
        },
        "db_instance_endpoint": {
            "sensitive": false,
            "type": "string",
            "value": "terraform-20230806213548663000000009.cmksk3juhzgm.ap-south-1.rds.amazonaws.com:5432"
        },
        "db_instance_password": {
            "sensitive": true,
            "type": "string",
            "value": "s47j7#&^c!mS8*hR"
        }
    },
    "state": "present",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "module.vpc.aws_eip.nat[1]: Refreshing state... [id=eipalloc-0cfc80c20f7391a69]\nmodule.eks.data.aws_iam_policy_document.cluster_assume_role_policy: Reading...\nmodule.eks.data.aws_iam_policy_document.cluster_elb_sl_role_creation[0]: Reading...\nmodule.eks.data.aws_caller_identity.current: Reading...\nmodule.vpc.aws_eip.nat[0]: Refreshing state... [id=eipalloc-0d57bf10630a1007d]\nmodule.vpc.aws_eip.nat[2]: Refreshing state... [id=eipalloc-0d8067addfb34f209]\nmodule.eks.data.aws_partition.current: Reading...\nmodule.vpc.aws_vpc.this[0]: Refreshing state... [id=vpc-08726f7641e60f1aa]\nmodule.eks.data.aws_iam_policy_document.cluster_elb_sl_role_creation[0]: Read complete after 0s [id=3709839417]\nmodule.eks.data.aws_partition.current: Read complete after 0s [id=aws]\nmodule.eks.data.aws_iam_policy_document.cluster_assume_role_policy: Read complete after 0s [id=2764486067]\nmodule.eks.aws_iam_policy.cluster_elb_sl_role_creation[0]: Refreshing state... [id=arn:aws:iam::072329308666:policy/cicd_pipeline_cluster-elb-sl-role-creation20230806213529231600000002]\nmodule.eks.data.aws_iam_policy_document.workers_assume_role_policy: Reading...\nmodule.eks.aws_iam_role.cluster[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001]\nmodule.eks.data.aws_iam_policy_document.workers_assume_role_policy: Read complete after 0s [id=3778018924]\nmodule.eks.data.aws_caller_identity.current: Read complete after 1s [id=072329308666]\nmodule.eks.aws_security_group.workers[0]: Refreshing state... [id=sg-0c797cb89244dc707]\nmodule.eks.aws_security_group.cluster[0]: Refreshing state... [id=sg-0bb10e125879cf48f]\naws_security_group.database_sg: Refreshing state... [id=sg-05096e67e741174b1]\nmodule.vpc.aws_route_table.private[0]: Refreshing state... [id=rtb-00844fde6307a4e31]\nmodule.vpc.aws_route_table.private[2]: Refreshing state... [id=rtb-0445245b3a6d8c140]\nmodule.vpc.aws_route_table.private[1]: Refreshing state... [id=rtb-07d2e555d634f64fd]\nmodule.vpc.aws_vpn_gateway.this[0]: Refreshing state... [id=vgw-077dd38ca73b2d7af]\nmodule.vpc.aws_subnet.private[2]: Refreshing state... [id=subnet-0c611c81b7a24fe33]\nmodule.vpc.aws_subnet.private[0]: Refreshing state... [id=subnet-02134020f4f1b8bbe]\nmodule.vpc.aws_subnet.private[1]: Refreshing state... [id=subnet-0a72582e9b3cfbc8d]\nmodule.vpc.aws_subnet.public[0]: Refreshing state... [id=subnet-078eb1d617d9dedd2]\nmodule.vpc.aws_subnet.public[2]: Refreshing state... [id=subnet-0445d50389cf1019a]\nmodule.vpc.aws_subnet.public[1]: Refreshing state... [id=subnet-0c9826ac864c667a7]\nmodule.vpc.aws_route_table.public[0]: Refreshing state... [id=rtb-01c7afa1bc774b14e]\nmodule.vpc.aws_internet_gateway.this[0]: Refreshing state... [id=igw-0f7a4cf88c08ce2f8]\nmodule.eks.aws_security_group_rule.cluster_egress_internet[0]: Refreshing state... [id=sgrule-3320559180]\nmodule.eks.aws_security_group_rule.workers_ingress_cluster[0]: Refreshing state... [id=sgrule-3140422957]\nmodule.eks.aws_security_group_rule.workers_ingress_cluster_https[0]: Refreshing state... [id=sgrule-311753983]\nmodule.eks.aws_security_group_rule.workers_egress_internet[0]: Refreshing state... [id=sgrule-3068130931]\nmodule.eks.aws_security_group_rule.cluster_https_worker_ingress[0]: Refreshing state... [id=sgrule-2922650210]\nmodule.eks.aws_security_group_rule.workers_ingress_self[0]: Refreshing state... [id=sgrule-1583484920]\nmodule.vpc.aws_route_table_association.private[0]: Refreshing state... [id=rtbassoc-032c23d47bced1ab8]\nmodule.vpc.aws_route_table_association.private[2]: Refreshing state... [id=rtbassoc-0c38900f1afe15c54]\nmodule.vpc.aws_route_table_association.private[1]: Refreshing state... [id=rtbassoc-0265bb39e9e8692e0]\naws_db_subnet_group.pipeline_database_sng: Refreshing state... [id=pipeline_database_sng]\nmodule.vpc.aws_route.public_internet_gateway[0]: Refreshing state... [id=r-rtb-01c7afa1bc774b14e1080289494]\nmodule.vpc.aws_nat_gateway.this[0]: Refreshing state... [id=nat-05d9cb282f3d434fb]\nmodule.vpc.aws_nat_gateway.this[1]: Refreshing state... [id=nat-0dd68818a4b143bf7]\nmodule.vpc.aws_nat_gateway.this[2]: Refreshing state... [id=nat-0830b8f5cc20d8576]\nmodule.vpc.aws_route_table_association.public[2]: Refreshing state... [id=rtbassoc-03872c863db60554c]\nmodule.vpc.aws_route_table_association.public[1]: Refreshing state... [id=rtbassoc-0274f2d895a9f9dda]\nmodule.vpc.aws_route_table_association.public[0]: Refreshing state... [id=rtbassoc-0b7678d10ee48bfbb]\nmodule.vpc.aws_route.private_nat_gateway[0]: Refreshing state... [id=r-rtb-00844fde6307a4e311080289494]\nmodule.vpc.aws_route.private_nat_gateway[1]: Refreshing state... [id=r-rtb-07d2e555d634f64fd1080289494]\nmodule.vpc.aws_route.private_nat_gateway[2]: Refreshing state... [id=r-rtb-0445245b3a6d8c1401080289494]\naws_db_instance.pipeline_database: Refreshing state... [id=terraform-20230806213548663000000009]\nmodule.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSServicePolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542490100000003]\nmodule.eks.aws_iam_role_policy_attachment.cluster_elb_sl_role_creation[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542552200000006]\nmodule.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542525300000005]\nmodule.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSVPCResourceControllerPolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542509400000004]\nmodule.eks.aws_eks_cluster.this[0]: Refreshing state... [id=cicd_pipeline_cluster]\nmodule.eks.data.http.wait_for_cluster[0]: Reading...\nmodule.eks.aws_iam_role.workers[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a]\nmodule.eks.local_file.kubeconfig[0]: Refreshing state... [id=d12ea07b9064d41fb066544dee368cc268aaa760]\nmodule.eks.data.http.wait_for_cluster[0]: Read complete after 1s [id=https://46053C82096527C9C95ED11748A3B3A0.gr7.ap-south-1.eks.amazonaws.com/healthz]\ndata.aws_eks_cluster.cluster: Reading...\ndata.aws_eks_cluster_auth.cluster: Reading...\ndata.aws_eks_cluster_auth.cluster: Read complete after 0s [id=cicd_pipeline_cluster]\ndata.aws_eks_cluster.cluster: Read complete after 0s [id=cicd_pipeline_cluster]\nmodule.eks.aws_iam_role_policy_attachment.workers_AmazonEKSWorkerNodePolicy[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431370520000000d]\nmodule.eks.aws_iam_role_policy_attachment.workers_AmazonEC2ContainerRegistryReadOnly[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431001900000000b]\nmodule.eks.aws_iam_role_policy_attachment.workers_AmazonEKS_CNI_Policy[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431001910000000c]\nmodule.eks.kubernetes_config_map.aws_auth[0]: Refreshing state... [id=kube-system/aws-auth]\nmodule.eks.module.node_groups.aws_eks_node_group.workers[\"eks_nodes\"]: Refreshing state... [id=cicd_pipeline_cluster:cicd_pipeline_cluster-eks_nodes2023080621431673920000000e]\n\nNo changes. Your infrastructure matches the configuration.\n\nTerraform has compared your real infrastructure against your configuration\nand found no differences, so no changes are needed.\n",
    "stdout_lines": [
        "module.vpc.aws_eip.nat[1]: Refreshing state... [id=eipalloc-0cfc80c20f7391a69]",
        "module.eks.data.aws_iam_policy_document.cluster_assume_role_policy: Reading...",
        "module.eks.data.aws_iam_policy_document.cluster_elb_sl_role_creation[0]: Reading...",
        "module.eks.data.aws_caller_identity.current: Reading...",
        "module.vpc.aws_eip.nat[0]: Refreshing state... [id=eipalloc-0d57bf10630a1007d]",
        "module.vpc.aws_eip.nat[2]: Refreshing state... [id=eipalloc-0d8067addfb34f209]",
        "module.eks.data.aws_partition.current: Reading...",
        "module.vpc.aws_vpc.this[0]: Refreshing state... [id=vpc-08726f7641e60f1aa]",
        "module.eks.data.aws_iam_policy_document.cluster_elb_sl_role_creation[0]: Read complete after 0s [id=3709839417]",
        "module.eks.data.aws_partition.current: Read complete after 0s [id=aws]",
        "module.eks.data.aws_iam_policy_document.cluster_assume_role_policy: Read complete after 0s [id=2764486067]",
        "module.eks.aws_iam_policy.cluster_elb_sl_role_creation[0]: Refreshing state... [id=arn:aws:iam::072329308666:policy/cicd_pipeline_cluster-elb-sl-role-creation20230806213529231600000002]",
        "module.eks.data.aws_iam_policy_document.workers_assume_role_policy: Reading...",
        "module.eks.aws_iam_role.cluster[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001]",
        "module.eks.data.aws_iam_policy_document.workers_assume_role_policy: Read complete after 0s [id=3778018924]",
        "module.eks.data.aws_caller_identity.current: Read complete after 1s [id=072329308666]",
        "module.eks.aws_security_group.workers[0]: Refreshing state... [id=sg-0c797cb89244dc707]",
        "module.eks.aws_security_group.cluster[0]: Refreshing state... [id=sg-0bb10e125879cf48f]",
        "aws_security_group.database_sg: Refreshing state... [id=sg-05096e67e741174b1]",
        "module.vpc.aws_route_table.private[0]: Refreshing state... [id=rtb-00844fde6307a4e31]",
        "module.vpc.aws_route_table.private[2]: Refreshing state... [id=rtb-0445245b3a6d8c140]",
        "module.vpc.aws_route_table.private[1]: Refreshing state... [id=rtb-07d2e555d634f64fd]",
        "module.vpc.aws_vpn_gateway.this[0]: Refreshing state... [id=vgw-077dd38ca73b2d7af]",
        "module.vpc.aws_subnet.private[2]: Refreshing state... [id=subnet-0c611c81b7a24fe33]",
        "module.vpc.aws_subnet.private[0]: Refreshing state... [id=subnet-02134020f4f1b8bbe]",
        "module.vpc.aws_subnet.private[1]: Refreshing state... [id=subnet-0a72582e9b3cfbc8d]",
        "module.vpc.aws_subnet.public[0]: Refreshing state... [id=subnet-078eb1d617d9dedd2]",
        "module.vpc.aws_subnet.public[2]: Refreshing state... [id=subnet-0445d50389cf1019a]",
        "module.vpc.aws_subnet.public[1]: Refreshing state... [id=subnet-0c9826ac864c667a7]",
        "module.vpc.aws_route_table.public[0]: Refreshing state... [id=rtb-01c7afa1bc774b14e]",
        "module.vpc.aws_internet_gateway.this[0]: Refreshing state... [id=igw-0f7a4cf88c08ce2f8]",
        "module.eks.aws_security_group_rule.cluster_egress_internet[0]: Refreshing state... [id=sgrule-3320559180]",
        "module.eks.aws_security_group_rule.workers_ingress_cluster[0]: Refreshing state... [id=sgrule-3140422957]",
        "module.eks.aws_security_group_rule.workers_ingress_cluster_https[0]: Refreshing state... [id=sgrule-311753983]",
        "module.eks.aws_security_group_rule.workers_egress_internet[0]: Refreshing state... [id=sgrule-3068130931]",
        "module.eks.aws_security_group_rule.cluster_https_worker_ingress[0]: Refreshing state... [id=sgrule-2922650210]",
        "module.eks.aws_security_group_rule.workers_ingress_self[0]: Refreshing state... [id=sgrule-1583484920]",
        "module.vpc.aws_route_table_association.private[0]: Refreshing state... [id=rtbassoc-032c23d47bced1ab8]",
        "module.vpc.aws_route_table_association.private[2]: Refreshing state... [id=rtbassoc-0c38900f1afe15c54]",
        "module.vpc.aws_route_table_association.private[1]: Refreshing state... [id=rtbassoc-0265bb39e9e8692e0]",
        "aws_db_subnet_group.pipeline_database_sng: Refreshing state... [id=pipeline_database_sng]",
        "module.vpc.aws_route.public_internet_gateway[0]: Refreshing state... [id=r-rtb-01c7afa1bc774b14e1080289494]",
        "module.vpc.aws_nat_gateway.this[0]: Refreshing state... [id=nat-05d9cb282f3d434fb]",
        "module.vpc.aws_nat_gateway.this[1]: Refreshing state... [id=nat-0dd68818a4b143bf7]",
        "module.vpc.aws_nat_gateway.this[2]: Refreshing state... [id=nat-0830b8f5cc20d8576]",
        "module.vpc.aws_route_table_association.public[2]: Refreshing state... [id=rtbassoc-03872c863db60554c]",
        "module.vpc.aws_route_table_association.public[1]: Refreshing state... [id=rtbassoc-0274f2d895a9f9dda]",
        "module.vpc.aws_route_table_association.public[0]: Refreshing state... [id=rtbassoc-0b7678d10ee48bfbb]",
        "module.vpc.aws_route.private_nat_gateway[0]: Refreshing state... [id=r-rtb-00844fde6307a4e311080289494]",
        "module.vpc.aws_route.private_nat_gateway[1]: Refreshing state... [id=r-rtb-07d2e555d634f64fd1080289494]",
        "module.vpc.aws_route.private_nat_gateway[2]: Refreshing state... [id=r-rtb-0445245b3a6d8c1401080289494]",
        "aws_db_instance.pipeline_database: Refreshing state... [id=terraform-20230806213548663000000009]",
        "module.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSServicePolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542490100000003]",
        "module.eks.aws_iam_role_policy_attachment.cluster_elb_sl_role_creation[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542552200000006]",
        "module.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542525300000005]",
        "module.eks.aws_iam_role_policy_attachment.cluster_AmazonEKSVPCResourceControllerPolicy[0]: Refreshing state... [id=cicd_pipeline_cluster20230806213529231200000001-20230806213542509400000004]",
        "module.eks.aws_eks_cluster.this[0]: Refreshing state... [id=cicd_pipeline_cluster]",
        "module.eks.data.http.wait_for_cluster[0]: Reading...",
        "module.eks.aws_iam_role.workers[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a]",
        "module.eks.local_file.kubeconfig[0]: Refreshing state... [id=d12ea07b9064d41fb066544dee368cc268aaa760]",
        "module.eks.data.http.wait_for_cluster[0]: Read complete after 1s [id=https://46053C82096527C9C95ED11748A3B3A0.gr7.ap-south-1.eks.amazonaws.com/healthz]",
        "data.aws_eks_cluster.cluster: Reading...",
        "data.aws_eks_cluster_auth.cluster: Reading...",
        "data.aws_eks_cluster_auth.cluster: Read complete after 0s [id=cicd_pipeline_cluster]",
        "data.aws_eks_cluster.cluster: Read complete after 0s [id=cicd_pipeline_cluster]",
        "module.eks.aws_iam_role_policy_attachment.workers_AmazonEKSWorkerNodePolicy[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431370520000000d]",
        "module.eks.aws_iam_role_policy_attachment.workers_AmazonEC2ContainerRegistryReadOnly[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431001900000000b]",
        "module.eks.aws_iam_role_policy_attachment.workers_AmazonEKS_CNI_Policy[0]: Refreshing state... [id=cicd_pipeline_cluster2023080621424825950000000a-2023080621431001910000000c]",
        "module.eks.kubernetes_config_map.aws_auth[0]: Refreshing state... [id=kube-system/aws-auth]",
        "module.eks.module.node_groups.aws_eks_node_group.workers[\"eks_nodes\"]: Refreshing state... [id=cicd_pipeline_cluster:cicd_pipeline_cluster-eks_nodes2023080621431673920000000e]",
        "",
        "No changes. Your infrastructure matches the configuration.",
        "",
        "Terraform has compared your real infrastructure against your configuration",
        "and found no differences, so no changes are needed."
    ],
    "workspace": "default"
}

TASK [Retrieve EKS kubeconfig] *************************************************
task path: /workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml:46
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: cicd-pipeline
<127.0.0.1> EXEC /bin/sh -c 'echo ~cicd-pipeline && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/cicd-pipeline/.ansible/tmp `"&& mkdir "` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974 `" && echo ansible-tmp-1691360255.9042194-182738-61875937737974="` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974 `" ) && sleep 0'
Using module file /home/cicd-pipeline/.local/lib/python3.10/site-packages/ansible/modules/command.py
<127.0.0.1> PUT /home/cicd-pipeline/.ansible/tmp/ansible-local-181374erbyjfj5/tmpbveqse39 TO /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974/AnsiballZ_command.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974/ /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '/home/cicd-pipeline/.python/current/bin/python3 /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974/AnsiballZ_command.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360255.9042194-182738-61875937737974/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => {
    "changed": false,
    "cmd": [
        "aws",
        "eks",
        "--region",
        "ap-south-1",
        "update-kubeconfig",
        "--name",
        "cicd_pipeline_cluster"
    ],
    "delta": "0:00:01.379742",
    "end": "2023-08-06 22:17:37.762771",
    "invocation": {
        "module_args": {
            "_raw_params": "aws eks --region ap-south-1 update-kubeconfig --name cicd_pipeline_cluster",
            "_uses_shell": false,
            "argv": null,
            "chdir": null,
            "creates": null,
            "executable": null,
            "removes": null,
            "stdin": null,
            "stdin_add_newline": true,
            "strip_empty_ends": true
        }
    },
    "msg": "",
    "rc": 0,
    "start": "2023-08-06 22:17:36.383029",
    "stderr": "",
    "stderr_lines": [],
    "stdout": "Updated context arn:aws:eks:ap-south-1:072329308666:cluster/cicd_pipeline_cluster in /home/cicd-pipeline/.kube/config",
    "stdout_lines": [
        "Updated context arn:aws:eks:ap-south-1:072329308666:cluster/cicd_pipeline_cluster in /home/cicd-pipeline/.kube/config"
    ]
}

TASK [Ensure namespaces exist] *************************************************
task path: /workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml:52
<127.0.0.1> ESTABLISH LOCAL CONNECTION FOR USER: cicd-pipeline
<127.0.0.1> EXEC /bin/sh -c 'echo ~cicd-pipeline && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/cicd-pipeline/.ansible/tmp `"&& mkdir "` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112 `" && echo ansible-tmp-1691360257.8302908-182771-206394541599112="` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112 `" ) && sleep 0'
Using module file /home/cicd-pipeline/.ansible/collections/ansible_collections/kubernetes/core/plugins/modules/k8s.py
<127.0.0.1> PUT /home/cicd-pipeline/.ansible/tmp/ansible-local-181374erbyjfj5/tmpurd2yelw TO /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112/AnsiballZ_k8s.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112/ /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112/AnsiballZ_k8s.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '/home/cicd-pipeline/.python/current/bin/python3 /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112/AnsiballZ_k8s.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360257.8302908-182771-206394541599112/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => (item={'name': 'django', 'chart_path': '/workspaces/cicd-pipeline-project/helm-charts/django-app', 'release_name': 'django-app', 'namespace': 'default', 'values_path': '/workspaces/cicd-pipeline-project/helm-charts/django-app/user-values.yaml.secrets'}) => {
    "ansible_loop_var": "item",
    "changed": false,
    "invocation": {
        "module_args": {
            "api_key": null,
            "api_version": "v1",
            "append_hash": false,
            "apply": false,
            "ca_cert": null,
            "client_cert": null,
            "client_key": null,
            "context": null,
            "continue_on_error": false,
            "delete_options": null,
            "force": false,
            "generate_name": null,
            "host": null,
            "impersonate_groups": null,
            "impersonate_user": null,
            "kind": "Namespace",
            "kubeconfig": "/home/cicd-pipeline/.kube/config",
            "label_selectors": null,
            "merge_type": null,
            "name": "default",
            "namespace": null,
            "no_proxy": null,
            "password": null,
            "persist_config": null,
            "proxy": null,
            "proxy_headers": null,
            "resource_definition": null,
            "server_side_apply": null,
            "src": null,
            "state": "present",
            "template": null,
            "username": null,
            "validate": null,
            "validate_certs": null,
            "wait": false,
            "wait_condition": null,
            "wait_sleep": 5,
            "wait_timeout": 120
        }
    },
    "item": {
        "chart_path": "/workspaces/cicd-pipeline-project/helm-charts/django-app",
        "name": "django",
        "namespace": "default",
        "release_name": "django-app",
        "values_path": "/workspaces/cicd-pipeline-project/helm-charts/django-app/user-values.yaml.secrets"
    },
    "method": "update",
    "result": {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {
            "creationTimestamp": "2023-08-06T21:40:33Z",
            "labels": {
                "kubernetes.io/metadata.name": "default"
            },
            "managedFields": [
                {
                    "apiVersion": "v1",
                    "fieldsType": "FieldsV1",
                    "fieldsV1": {
                        "f:metadata": {
                            "f:labels": {
                                ".": {},
                                "f:kubernetes.io/metadata.name": {}
                            }
                        }
                    },
                    "manager": "kube-apiserver",
                    "operation": "Update",
                    "time": "2023-08-06T21:40:33Z"
                }
            ],
            "name": "default",
            "resourceVersion": "37",
            "uid": "67d9b805-03e4-4c33-918f-a053dad4f921"
        },
        "spec": {
            "finalizers": [
                "kubernetes"
            ]
        },
        "status": {
            "phase": "Active"
        }
    }
}
<127.0.0.1> EXEC /bin/sh -c 'echo ~cicd-pipeline && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '( umask 77 && mkdir -p "` echo /home/cicd-pipeline/.ansible/tmp `"&& mkdir "` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054 `" && echo ansible-tmp-1691360260.4297774-182771-170769239135054="` echo /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054 `" ) && sleep 0'
Using module file /home/cicd-pipeline/.ansible/collections/ansible_collections/kubernetes/core/plugins/modules/k8s.py
<127.0.0.1> PUT /home/cicd-pipeline/.ansible/tmp/ansible-local-181374erbyjfj5/tmp15cx95wd TO /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054/AnsiballZ_k8s.py
<127.0.0.1> EXEC /bin/sh -c 'chmod u+x /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054/ /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054/AnsiballZ_k8s.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c '/home/cicd-pipeline/.python/current/bin/python3 /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054/AnsiballZ_k8s.py && sleep 0'
<127.0.0.1> EXEC /bin/sh -c 'rm -f -r /home/cicd-pipeline/.ansible/tmp/ansible-tmp-1691360260.4297774-182771-170769239135054/ > /dev/null 2>&1 && sleep 0'
ok: [localhost] => (item={'name': 'monitoring', 'chart_path': '/workspaces/cicd-pipeline-project/helm-charts/monitoring-stack', 'release_name': 'monitoring-stack', 'namespace': 'monitoring', 'values_path': '/workspaces/cicd-pipeline-project/helm-charts/monitoring-stack/user-values.yaml.secrets'}) => {
    "ansible_loop_var": "item",
    "changed": false,
    "invocation": {
        "module_args": {
            "api_key": null,
            "api_version": "v1",
            "append_hash": false,
            "apply": false,
            "ca_cert": null,
            "client_cert": null,
            "client_key": null,
            "context": null,
            "continue_on_error": false,
            "delete_options": null,
            "force": false,
            "generate_name": null,
            "host": null,
            "impersonate_groups": null,
            "impersonate_user": null,
            "kind": "Namespace",
            "kubeconfig": "/home/cicd-pipeline/.kube/config",
            "label_selectors": null,
            "merge_type": null,
            "name": "monitoring",
            "namespace": null,
            "no_proxy": null,
            "password": null,
            "persist_config": null,
            "proxy": null,
            "proxy_headers": null,
            "resource_definition": null,
            "server_side_apply": null,
            "src": null,
            "state": "present",
            "template": null,
            "username": null,
            "validate": null,
            "validate_certs": null,
            "wait": false,
            "wait_condition": null,
            "wait_sleep": 5,
            "wait_timeout": 120
        }
    },
    "item": {
        "chart_path": "/workspaces/cicd-pipeline-project/helm-charts/monitoring-stack",
        "name": "monitoring",
        "namespace": "monitoring",
        "release_name": "monitoring-stack",
        "values_path": "/workspaces/cicd-pipeline-project/helm-charts/monitoring-stack/user-values.yaml.secrets"
    },
    "method": "update",
    "result": {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {
            "creationTimestamp": "2023-08-06T22:08:14Z",
            "labels": {
                "kubernetes.io/metadata.name": "monitoring"
            },
            "managedFields": [
                {
                    "apiVersion": "v1",
                    "fieldsType": "FieldsV1",
                    "fieldsV1": {
                        "f:metadata": {
                            "f:labels": {
                                ".": {},
                                "f:kubernetes.io/metadata.name": {}
                            }
                        }
                    },
                    "manager": "OpenAPI-Generator",
                    "operation": "Update",
                    "time": "2023-08-06T22:08:14Z"
                }
            ],
            "name": "monitoring",
            "resourceVersion": "4566",
            "uid": "0d9dae5b-d186-4645-a589-cd0adf0c4a7f"
        },
        "spec": {
            "finalizers": [
                "kubernetes"
            ]
        },
        "status": {
            "phase": "Active"
        }
    }
}

TASK [Install Helm chart for {{ item.name }}] **********************************
task path: /workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml:63
fatal: [localhost]: FAILED! => {
    "msg": "The task includes an option with an undefined variable. The error was: 'present' is undefined. 'present' is undefined\n\nThe error appears to be in '/workspaces/cicd-pipeline-project/ansible/playbooks/helm-charts.yml': line 63, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n    # Deploying Helm Charts Using a Loop\n    - name: \"Install Helm chart for {{ item.name }}\"\n      ^ here\nWe could be wrong, but this one looks like it might be an issue with\nmissing quotes. Always quote template expression brackets when they\nstart a value. For instance:\n\n    with_items:\n      - {{ foo }}\n\nShould be written as:\n\n    with_items:\n      - \"{{ foo }}\"\n"
}

PLAY RECAP *********************************************************************
localhost                  : ok=4    changed=0    unreachable=0    failed=1    skipped=0    rescued=0    ignored=0   

